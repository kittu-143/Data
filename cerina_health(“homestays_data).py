# -*- coding: utf-8 -*-
"""Cerina Health(â€œHomestays_Data).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gorXHgN5rCr1wKV4lYrJNRPXpZ5PyQZa
"""

!pip install openpyxl
!pip install pandas openpyxl
!pip install xlrd
!pip install textblob
!pip install shap

# 1. Feature Engineering:

import pandas as pd
import datetime

# Load the dataset
homestays_data = pd.read_excel("Homestays_Data.xlsx")

# Calculate the Host_Tenure feature
homestays_data['host_since'] = pd.to_datetime(homestays_data['host_since'])
homestays_data['Host_Tenure'] = (datetime.datetime.now() - homestays_data['host_since']).dt.days / 365

# Calculate the Amenities_Count feature
homestays_data['Amenities_Count'] = homestays_data['amenities'].apply(lambda x: len(x.split(',')))

# Calculate the Days_Since_Last_Review feature
homestays_data['last_review'] = pd.to_datetime(homestays_data['last_review'])
homestays_data['Days_Since_Last_Review'] = (datetime.datetime.now() - homestays_data['last_review']).dt.days

# Drop the original columns
homestays_data.drop(['host_since', 'last_review', 'amenities'], axis=1, inplace=True)

# Save the updated dataset
homestays_data.to_excel("Updated_Homestays_Data.xlsx", index=False)
print("Updated dataset saved successfully.")

# 2. Exploratory Data Analysis (EDA):
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
homestays_data = pd.read_excel("Updated_Homestays_Data.xlsx")

# Remove rows with non-numeric values in the 'log_price' column
homestays_data = homestays_data[pd.to_numeric(homestays_data['log_price'], errors='coerce').notnull()]

# Analyze correlations
correlation_matrix = homestays_data[['log_price', 'accommodates', 'number_of_reviews']].corr()
print("Correlation Matrix:")
print(correlation_matrix)

# Create histograms
for col in ['log_price', 'accommodates', 'number_of_reviews']:
    plt.figure(figsize=(8, 6))
    sns.histplot(homestays_data[col], bins=20, kde=True)
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.title(f"Histogram of {col}")
    plt.show()

# Create scatter plots
plt.figure(figsize=(8, 6))
sns.scatterplot(x="log_price", y="accommodates", data=homestays_data)
plt.xlabel("log_price")
plt.ylabel("accommodates")
plt.title("Scatter Plot: log_price vs accommodates")
plt.show()

plt.figure(figsize=(8, 6))
sns.scatterplot(x="log_price", y="number_of_reviews", data=homestays_data)
plt.xlabel("log_price")
plt.ylabel("number_of_reviews")
plt.title("Scatter Plot: log_price vs number_of_reviews")
plt.show()

# 3. Geospatial Analysis:
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt

# Load the updated homestay dataset
homestays_data = pd.read_excel("Updated_Homestays_Data.xlsx")

# Convert log_price to numeric, handling errors
homestays_data['log_price'] = pd.to_numeric(homestays_data['log_price'], errors='coerce')

# Drop rows with missing log_price
homestays_data = homestays_data.dropna(subset=['log_price'])

# Convert the dataset to a GeoDataFrame
geometry = gpd.points_from_xy(homestays_data["longitude"], homestays_data["latitude"])
homestays_gdf = gpd.GeoDataFrame(homestays_data, geometry=geometry)

# Plot the listings on a map
homestays_gdf.plot(marker="o", color="blue", alpha=0.5)
plt.title("Homestay Listings in the Study Area")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.show()

# Analyze regional pricing trends
average_log_price_by_neighbourhood = homestays_gdf.groupby("neighbourhood")["log_price"].mean()
average_log_price_by_neighbourhood.plot(kind="bar")
plt.title("Average Log Price by Neighbourhood")
plt.xlabel("Neighbourhood")
plt.ylabel("Average Log Price")
plt.show()

# Calculate the correlation between latitude/longitude and log price
correlation_matrix = homestays_gdf[["log_price", "latitude", "longitude"]].corr()
print(correlation_matrix)

# 4. Sentiment Analysis on Textual Data:
import pandas as pd
from textblob import TextBlob

# Load the updated homestay dataset
homestays_data = pd.read_excel("Updated_Homestays_Data.xlsx")

# Check the data types of the description column
description_types = homestays_data["description"].dtypes

# Convert any non-string values to strings
if description_types != "object":
    homestays_data["description"] = homestays_data["description"].astype(str)

# Extract the description column
descriptions = homestays_data["description"]

# Create a new column for sentiment scores
homestays_data["sentiment_score"] = 0

# Iterate through each description and calculate the sentiment score
for i, description in enumerate(descriptions):
    # Skip NaN values
    if isinstance(description, float) and pd.isna(description):
        continue
    text_blob = TextBlob(description)
    homestays_data.loc[i, "sentiment_score"] = text_blob.sentiment.polarity

# Analyze the sentiment scores
average_sentiment_score = homestays_data["sentiment_score"].mean()
print(f"Average sentiment score: {average_sentiment_score}")

# Identify the most positive and negative descriptions
most_positive_description = homestays_data.loc[homestays_data["sentiment_score"].idxmax(), "description"]
most_negative_description = homestays_data.loc[homestays_data["sentiment_score"].idxmin(), "description"]
print(f"Most negative description: {most_negative_description}")
print(f"Most positive description: {most_positive_description}")

# 5. Amenities Analysis:
import pandas as pd

# Load the updated homestay dataset
homestays_data = pd.read_excel("Updated_Homestays_Data.xlsx")

# Create a list of all unique amenities
all_amenities = set()

# Convert amenities to a list of strings
homestays_data["Amenities_Count"] = homestays_data["Amenities_Count"].astype(str)
homestays_data["Amenities_Count"] = homestays_data["Amenities_Count"].str.split(",")

# Iterate through each homestay and update the counts and prices
for amenities in homestays_data["Amenities_Count"]:
    all_amenities.update(amenities)

# Create a dictionary to store amenity counts and average prices
amenity_counts = {amenity: 0 for amenity in all_amenities}
amenity_prices = {amenity: [] for amenity in all_amenities}

# Iterate through each homestay and update the counts and prices
for index, row in homestays_data.iterrows():
    amenities = row["Amenities_Count"]
    price = row["log_price"]
    for amenity in amenities:
        amenity_counts[amenity.strip()] += 1
        amenity_prices[amenity.strip()].append(price)

# Calculate average price for each amenity
average_prices = {amenity: sum(prices) / amenity_counts[amenity] for amenity, prices in amenity_prices.items()}

# Sort amenities by average price
sorted_amenities = sorted(average_prices.items(), key=lambda x: x[1], reverse=True)

# Display top 5 and bottom 5 amenities associated with prices
print("Top 5 amenities associated with higher prices:")
for amenity, price in sorted_amenities[:5]:
    print(f"\t- {amenity}: Average Price {price:.2f}")

print("\nBottom 5 amenities associated with lower prices:")
for amenity, price in sorted_amenities[-5:]:
    print(f"\t- {amenity}: Average Price {price:.2f}")

# 6. Categorical Data Encoding:

import pandas as pd

# Load the updated homestay dataset
homestays_data = pd.read_excel("Updated_Homestays_Data.xlsx")

# Define categorical variables to encode
categorical_variables = ["room_type", "city", "property_type"]

# Perform one-hot encoding
homestays_data = pd.get_dummies(homestays_data, columns=categorical_variables, drop_first=True)

# Print the updated DataFrame with encoded variables
print(homestays_data.head())

# 7. Model Development and Training:
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error

# Load the updated homestay dataset
homestays_data = pd.read_excel("Updated_Homestays_Data.xlsx")

# Select features (independent variables) and target (dependent variable)
features = ['accommodates', 'bathrooms', 'bedrooms', 'beds', 'Amenities_Count', 'Host_Tenure', 'Days_Since_Last_Review']
target = 'log_price'

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(homestays_data[features], homestays_data[target], test_size=0.2, random_state=42)

# Handle missing values with SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Baseline Linear Regression model
linear_reg_model = LinearRegression()
linear_reg_model.fit(X_train_imputed, y_train)
linear_reg_predictions = linear_reg_model.predict(X_test_imputed)
linear_reg_rmse = mean_squared_error(y_test, linear_reg_predictions, squared=False)
print("Linear Regression RMSE:", linear_reg_rmse)

# RandomForestRegressor model
random_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)
random_forest_model.fit(X_train_imputed, y_train)
random_forest_predictions = random_forest_model.predict(X_test_imputed)
random_forest_rmse = mean_squared_error(y_test, random_forest_predictions, squared=False)
print("Random Forest RMSE:", random_forest_rmse)

# GradientBoostingRegressor model
gradient_boosting_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
gradient_boosting_model.fit(X_train_imputed, y_train)
gradient_boosting_predictions = gradient_boosting_model.predict(X_test_imputed)
gradient_boosting_rmse = mean_squared_error(y_test, gradient_boosting_predictions, squared=False)
print("Gradient Boosting RMSE:", gradient_boosting_rmse)

# 8. Model Optimization and Validation:
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

# Define models and their parameter grids for grid search
models = {
    'LinearRegression': (LinearRegression(), {}),
    'RandomForestRegressor': (RandomForestRegressor(), {'model__n_estimators': [50, 100, 150]}),  # Specify 'model__n_estimators' within RandomForestRegressor
    'GradientBoostingRegressor': (GradientBoostingRegressor(), {'model__n_estimators': [50, 100, 150], 'model__max_depth': [3, 5, 7], 'model__learning_rate': [0.05, 0.1, 0.2]})
}

# Perform grid search and cross-validation for each model
for name, (model, param_grid) in models.items():
    print(f"Training {name}...")

    # Define pipeline with imputer
    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),  # Replace missing values with mean
        ('scaler', StandardScaler()),
        ('model', model)
    ])

    grid_search = GridSearchCV(pipeline, param_grid, cv=KFold(n_splits=5, shuffle=True, random_state=42), scoring='neg_mean_squared_error')
    grid_search.fit(X_train, y_train)
    best_params = grid_search.best_params_
    best_score = -grid_search.best_score_
    print(f"Best parameters: {best_params}")
    print(f"Best mean squared error: {best_score}")

    # Evaluate the model on the test set
    y_pred = grid_search.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    print(f"Test mean squared error: {mse}")
    print("")

# 9. Feature Importance and Model Insights:
import pandas as pd
import matplotlib.pyplot as plt
import shap  # Import SHAP library

# Accessing feature importance for RandomForestRegressor
random_forest_model = grid_search.best_estimator_  # Assuming `grid_search` contains the trained RandomForestRegressor model
feature_importances = pd.Series(random_forest_model['model'].feature_importances_, index=features)
feature_importances.sort_values(ascending=False).plot(kind='bar')
plt.title('Feature Importances for RandomForestRegressor')
plt.xlabel('Features')
plt.ylabel('Importance')
plt.show()

# Accessing SHAP values for GradientBoostingRegressor
shap_values = shap.TreeExplainer(grid_search.best_estimator_['model']).shap_values(X_train)
shap.summary_plot(shap_values, X_train, plot_type='bar')
plt.title('SHAP Feature Importance for GradientBoostingRegressor')
plt.xlabel('SHAP Value (mean absolute)')
plt.show()

# 10. Predictive Performance Assessment:
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Assuming `grid_search` contains the trained model
best_model = grid_search.best_estimator_

# Evaluate the final model on the reserved test set
y_pred = best_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Test RMSE: {rmse}")
print(f"Test R-squared: {r2}")

# Analyze the residuals
residuals = y_test - y_pred

# Plot the residuals vs. predicted values
plt.scatter(y_pred, residuals)
plt.axhline(0, color='red')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residuals vs. Predicted Values')
plt.show()

# Check for patterns in the residuals
sns.histplot(residuals)
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Distribution of Residuals')
plt.show()

# Calculate the mean and standard deviation of the residuals
mean_residual = np.mean(residuals)
std_residual = np.std(residuals)

print(f"Mean residual: {mean_residual}")
print(f"Standard deviation of residuals: {std_residual}")

# Interpret the results
# - If the RMSE and R-squared values are acceptable for your problem domain, then the model is performing well.
# - If the residuals are randomly scattered around the zero line, then there is no evidence of model bias.
# - If the residuals show any patterns (e.g., increasing variance with increasing predicted values), then the model may need to be improved.